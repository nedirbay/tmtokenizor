# üáπüá≤ T√ºrkmen BPE Tokenizer (Byte-Pair Encoding)

T√ºrkmen dilini≈à textini token'lere b√∂len intelligent tokenizer. Bu proje Byte-Pair Encoding (BPE) algoritmini T√ºrkmen diline √∂zel adaptasi√Ωa bilen durmu≈üa ge√ßir√Ω√§r.

## üìã Mazmun

- [A√Ωratynlyklar](#a√Ωratynlyklar)
- [Tehnologi√Ωa](#tehnologi√Ωa)
- [√ù√ºklemek we Ulanmak](#√Ω√ºklemek-we-ulanmak)
- [Tokenizer Komponenti](#tokenizer-komponenti)
- [Mysallary](#mysallary)
- [Parametrler we Sazlamalar](#parametrler-we-sazlamalar)

---

## ‚ú® A√Ωratynlyklar

### 1. **Byte-Pair Encoding (BPE) Algoritmi**

- T√§ze s√∂z kitaby (vocabulary) √∂wren√Ω√§r
- I≈à √Ωygy token j√ºb√ºtlerini birle≈üdirerek token sany optimalla≈üdyr√Ωar
- 10,000 token √ßenli √Ωygyn texti ge√ßirm√§ge uny√Ωan

### 2. **T√ºrkmen Diline √ñzel Terjimeler**

- **Harfsƒ±z Harplary Golda√Ωan**: `√§`, `≈à`, `√∂`, `≈ü`, `√º`, `√Ω`, `≈æ` - guramotory T√ºrkmen dialektasy tarapyndan ulanyl√Ωan a√Ωratyn ny≈üanlar
- **T√ºrkmen Go≈üulmalaryny≈à Analizi**: S√∂zleri≈à `lary≈à`, `syzlyk`, `darlyk`, `maly` we be√Ωleki go≈üulmalary a√Ωratynla√Ωar
- **Geografik N√§meleri Golda√Ωan**:
  - 30+ T√ºrkmenistan ≈ü√§her we wela√Ωat
  - 50+ t√§ze atlaryny≈à (erkek, a√Ωal)
  - 20+ da≈üary √Ωurt atlary

### 3. **Adaty Atlar Klassifikasi√Ωasy**

Tokenizer a≈üakdaky at g√∂rn√º≈ülerini ilenip sa√Ωla√Ωar:

- **Adam Atlary** (≈üahsy): 60+ erkek atly, 30+ a√Ωal atly
- **≈û√§her Atlary**: A≈ügabat, Da≈üoguz, Balkanabat, Mary, Tejen we ba≈ügalar
- **√ùurt Atlary**: T√ºrki√Ωe, E√Ωran, Russi√Ωa, Hyta√Ω we ba≈ügalar
- **M√∂h√ºm S√∂zler**: "T√ºrkmenistan", "Prezident", "Halk", "Gara≈üsyzlyk"

### 4. **A√Ωratyn Tokenler**

Modelleme √º√ßin 13 a√Ωratyn token:

```
<pad>      - Doldurgy√ß
<unk>      - Tany≈üsyz token
<bos>      - Sentensi√Ωa ba≈üy
<eos>      - Sentensi√Ωa so≈ày
<mask>     - Maskir (MLM √º√ßin)
<cls>      - Klassifikasi√Ωa ba≈üy
<sep>      - Ayyryjy
<name>     - Adam ady
<city>     - ≈û√§her ady
<country>  - √ùurt ady
<num>      - Sany
<url>      - Web salgysy
<email>    - E-po√ßta
```

### 5. **Teksti Normalla≈üdyrma**

- NFKC unicode normalla≈üdyryly≈üy
- Kesgitlenen √Ωal≈ày≈ü kodlanyl≈ülar d√ºzetmek
- Artykma√ß bo≈üluklary a√Ωyrmak
- Lowecase'e √∂w√ºrme

### 6. **Go≈üulmany B√∂l√ºp Ayyrma**

S√∂zleri≈à so≈àundaky T√ºrkmen go≈üulmasyny intellektual bilen a√Ωratynla√Ωar:

- `mekdepde` ‚Üí `mekdep` + `de`
- `kitaplarym` ‚Üí `kitap` + `lar` + `ym`
- `g√∂zelliksi≈æ` ‚Üí `g√∂zelli` + `k` + `si≈æ`

---

## üõ† Tehnologi√Ωa

### Asasy Python Kitaphanasy

- `collections` - Counter we defaultdict √º√ßin
- `unicodedata` - Unicode normalla≈üdyryly≈üy
- `json` - Tokenizer saklamak we √Ω√ºklemek
- `regex (re)` - Pattern matching we tokenla≈üdyrma
- `typing` - Tipleri barlamak

### Isteglere gora kitaphanalar

- `tokenizers` (Hugging Face) - Export √º√ßin

---

## üöÄ √ù√ºklemek we Ulanmak

### 1. Corpus'da √ñwretmek

```python
from bpetokenizer import TurkmenBPETokenizer

# Tokenizer d√∂ret (10,000 token i≈ü sayy)
tokenizer = TurkmenBPETokenizer(vocab_size=10000)

# Corpus'da √∂wret
tokenizer.train("dataset_AB_220524.txt", verbose=True)

# Saklama
tokenizer.save("turkmen_tokenizer.json")
```

### 2. Teksti Tokenla≈üdyrma

```python
# Tokenlere b√∂l
text = "Ahmet A≈ügabatda i≈üle√Ω√§r"
tokens = tokenizer.tokenize(text)
print(tokens)
# Netije: ['ahmet</w>', 'a≈ügabat</w>', 'da', 'i≈üle', '√Ω√§r</w>']

# Token ID-lerine √∂w√ºr (Model giri≈üi √º√ßin)
token_ids = tokenizer.encode(text)
print(token_ids)
# Netije: [245, 128, 82, 439, 521]

# ID-lerden tekste √∂w√ºr (Model √ßyky≈üy √º√ßin)
decoded = tokenizer.decode(token_ids)
print(decoded)
# Netije: "ahmet a≈ügabat da i≈üle √Ω√§r"
```

### 3. Tokenizer √ù√ºklemek

```python
# √ñwrenilen tokenizer-i √Ω√ºkle
tokenizer = TurkmenBPETokenizer()
tokenizer.load("turkmen_tokenizer.json")

# Derrew ulan
tokens = tokenizer.tokenize("Oguljan Mary ≈ü√§herinden geldi")
```

### 4. Hugging Face Formatyna Ge√ßirmek

```python
# Uly modeller (BERT, GPT) bilen ulanmak √º√ßin
tokenizer.export_to_huggingface("turkmen_hf_tokenizer.json")
```

---

## üîß Tokenizer Komponenti

### Esasy Klasslar we Metodlar

#### **`__init__(vocab_size: int = 10000)`**

Tokenizer ba≈ülat√Ωar. T√ºrkmen dilini≈à a√Ωratyn ny≈üanlaryny, at toplumlary we go≈üulmalary ba≈ülangy√ß sazla√Ωar.

**Parametrler:**

- `vocab_size`: Maksimal token sany (default: 10,000)

---

#### **`normalize_text(text: str) -> str`**

Teksti normalla≈üdyr√Ωar: Unicode NFKC, √Ωal≈ày≈ü kod d√ºzeltmeleri, artykma√ß bo≈üluk a√Ωyryly≈üy.

**Mysaly:**

```python
text = "  AHMET   a≈üGABAT  "
normalized = tokenizer.normalize_text(text)
# "ahmet a≈ügabat"
```

---

#### **`aggressive_suffix_split(word: str) -> str`**

S√∂zleri≈à so≈àundaky T√ºrkmen go≈üulmasyny a√Ωratynla√Ωar.

**Mysaly:**

```python
result = tokenizer.aggressive_suffix_split("mekdepde")
# "mekdep de"
```

---

#### **`pre_tokenize(text: str) -> List[Tuple[str, str]]`**

Teksti s√∂zlere we ny≈üanlara b√∂l√ºp, at g√∂rn√º≈üini belle√Ω√§r.

**Netije Format:**

```python
[
    ("ahmet", "name"),           # Adam ady
    ("a≈ügabat", "city"),         # ≈û√§her ady
    ("da", "word"),              # Ady s√∂z
    ("i≈üle", "word"),            # Ady s√∂z
    ("√Ω√§r", "word")              # Ady s√∂z
]
```

---

#### **`is_proper_noun(word: str) -> Tuple[bool, str]`**

S√∂zi≈à adaty at bardygyny barla√Ωar.

**Netije:**

```python
(is_proper, noun_type)
# Meselem: (True, 'name'), (False, None)
```

---

#### **`train(corpus_path: str, verbose: bool = True)`**

Corpus'dan tokenizer √∂wred√Ω√§r. BPE algoritmi ulan√Ωar.

**I≈üleme Tahminawy:**

1. A√Ωratyn tokenleri go≈ü
2. S√∂zleri≈à √Ωygylyklaryny hasapla
3. Ba≈ülangy√ß harp toplumy d√∂ret
4. Adaty atlary go≈ü
5. BPE birle≈üdirmelerini √Ωerine √Ωet
6. S√∂z kitabyny finalla≈üdyr

---

#### **`tokenize(text: str) -> List[str]`**

Teksti tokenlere b√∂l√Ω√§r.

**Mysaly:**

```python
text = "Turkmenistan we T√ºrki√Ωe dostlukly √Ωurtlar"
tokens = tokenizer.tokenize(text)
# Netije: ['turkmenistan</w>', 've</w>', 't√ºrk', 'i√Ωe</w>', 'do', 'st', 'luk', 'ly</w>', '√Ωurt', 'lar</w>']
```

---

#### **`encode(text: str) -> List[int]`**

Teksti token ID-lerine √∂w√ºr√Ω√§r (ML model giri≈üi).

**Mysaly:**

```python
token_ids = tokenizer.encode("Ahmet i≈üle√Ω√§r")
# [245, 82, 439, 521]
```

---

#### **`decode(token_ids: List[int]) -> str`**

Token ID-lerini tekste √∂w√ºr√Ω√§r (ML model √ßyky≈üy).

**Mysaly:**

```python
decoded = tokenizer.decode([245, 82, 439, 521])
# "ahmet i≈üle √Ω√§r"
```

---

#### **`save(filepath: str)`** / **`load(filepath: str)`**

Tokenizerini JSON fa√Ωlynda sakla√Ωar we √Ω√ºkle√Ω√§r.

**JSON Toplam:**

```json
{
  "vocab": {...},
  "merges": [...],
  "vocab_size": 10000,
  "special_tokens": {...},
  "male_names": [...],
  "female_names": [...],
  "cities": [...],
  "countries": [...],
  "important_words": [...]
}
```

---

#### **`add_names(names: List[str], gender: str = 'male')`**

√ñwreni≈üden so≈à t√§ze atlary go≈ü√Ωar.

**Mysaly:**

```python
tokenizer.add_names(['Serdar', 'D√∂wlet'], gender='male')
tokenizer.add_names(['Maral', 'Le√Ωla'], gender='female')
```

---

#### **`add_cities(cities: List[str])`**

T√§ze ≈ü√§her atlaryny go≈ü√Ωar.

**Mysaly:**

```python
tokenizer.add_cities(['Tejen', 'Ba√Ωramaly'])
```

---

#### **`export_to_huggingface(save_path: str = "turkmen_hf_tokenizer.json")`**

Tokenizerini Hugging Face formatyna ge√ßirerek sakla√Ωar. BERT, GPT we be√Ωleki uly modeller bilen ulanmak √º√ßin ideal.

**Mizady:**

```python
tokenizer.export_to_huggingface("my_hf_tokenizer.json")
```

---

## üìä Teksti Analizi we At Klassifikasi√Ωasy

Tokenizer, teksti tokenla≈üdyrmagyny≈à bilen hasada ≈üu analyti ber√Ω√§r:

```python
text = "Magtymguly A≈ügabatda Prezidentligi≈à b√ºrosynda i≈üle√Ω√§rdi"
typed_tokens = tokenizer.pre_tokenize(text)

# Netije:
# [
#     ("magtymguly", "name"),
#     ("a≈ügabat", "city"),
#     ("da", "word"),
#     ...
# ]
```

**Klassifikasion G√∂rn√º≈üleri:**

- `name` - Adam ady
- `city` - ≈û√§her ady
- `country` - √ùurt ady
- `important` - M√∂h√ºm s√∂z
- `word` - Ady s√∂z

---

## üìù Mysallary

### Synag 1: Adam Atlary

```python
text = "Ahmet Muhammet we Oguljan A≈ügabatda"
tokens = tokenizer.tokenize(text)
# Netije: ['ahmet</w>', 'muhammet</w>', 've</w>', 'oguljan</w>', 'a≈ügabat</w>', 'da']
```

### Synag 2: Geografik Atlary

```python
text = "T√ºrki√Ωe Russi√Ωa E√Ωran we Hyta√Ω"
tokens = tokenizer.tokenize(text)
# Netije: ['t√ºrki√Ωe</w>', 'russi√Ωa</w>', 'e√Ωran</w>', 've</w>', 'hyta√Ω</w>']
```

### Synag 3: Go≈üulmaly S√∂zler

```python
text = "mekdepde kitaplarym"
tokens = tokenizer.tokenize(text)
# Netije: ['mekdep', 'de', 'kitap', 'lar', 'ym</w>']
```

### Synag 4: Kompleks S√∂z Kompleksi

```python
text = "T√ºrkmenistan Prezidentligi≈à Portaly"
tokens = tokenizer.tokenize(text)
encoded = tokenizer.encode(text)
decoded = tokenizer.decode(encoded)

print("Asly:", text)
print("Tokenler:", tokens)
print("ID-ler:", encoded)
print("G√∂rn√º≈ü:", decoded)
```

---

## ‚öôÔ∏è Parametrler we Sazlamalar

### Ba≈ülangy√ß Sazlamalar

```python
class TurkmenBPETokenizer:
    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = 10000          # Maksimal token sany
        self.vocab = {}                  # Token -> ID s√∂zl√ºgi
        self.merges = []                 # BPE birle≈üdirme taryhy
        self.word_freqs = {}             # S√∂z √Ωygylygy
        # ... be√Ωlekiler
```

### T√ºrkmen Harplary

```python
self.turkmen_chars = set('√§≈à√∂≈ü√º√Ω≈æ')
```

### A√Ωratyn Tokenler ID-leri

| Token       | ID  | H√§si√Ωeti        |
| ----------- | --- | --------------- |
| `<pad>`     | 0   | Doldurgy√ß       |
| `<unk>`     | 1   | Tany≈üsyz        |
| `<bos>`     | 2   | Sentensi√Ωa ba≈üy |
| `<eos>`     | 3   | Sentensi√Ωa so≈ày |
| `<mask>`    | 4   | Maskir          |
| `<cls>`     | 5   | Klassifikasi√Ωa  |
| `<sep>`     | 6   | Ayyryjy         |
| `<name>`    | 7   | Adam ady        |
| `<city>`    | 8   | ≈û√§her ady       |
| `<country>` | 9   | √ùurt ady        |
| `<num>`     | 10  | Sany            |
| `<url>`     | 11  | Web salgysy     |
| `<email>`   | 12  | E-po√ßta         |

---

## üìä √ñwreni≈ü Prosesi (Training Process)

BPE algoritmi a≈üakdaky √§dimler bilen i≈üle√Ω√§r:

### 1. **Ba≈ülangy√ß Sazlama**

- A√Ωratyn tokenleri go≈ü (13 token)
- Corpus'dan s√∂zleri≈à √Ωygylyklaryny hasapla
- Her s√∂zi harplaryna b√∂l: `mekdep` ‚Üí `m`, `e`, `k`, `d`, `e`, `p</w>`

### 2. **J√ºb√ºt √ùygylygy Hasaplama**

ƒ∞√Ωli≈üik harplar j√ºb√ºtlerini≈à √Ωygylygyny tap√Ωar.

### 3. **ƒ∞≈à √ùygy J√ºb√ºt Birle≈üdirmek**

I≈à √Ωygy j√ºb√ºti birle≈üdirip, t√§ze token d√∂red√Ω√§r.

### 4. **Tekrarlama**

`vocab_size - len(initial_tokens)` sa√Ωy √º√ßin 2-3 √§dim tekrarlany≈àar.

### 5. **Finalla≈üdyrma**

S√∂z kitaby (vocabulary) d√∂redil√Ω√§r we saklan√Ωar.

---

## üíæ Fa√Ωllary D√º≈ü√ºnmek

### `bpetokenizer.py`

Esasy `TurkmenBPETokenizer` klassy we √§hli metodlar.

### `usetokenizer.py`

Tokenizerini praktikal ulany≈ü mysallary.

### `dataset_AB_220524.txt`

√ñwreni≈üi √º√ßin corpus (A we B toplumlary birle≈üdirilen).

### `turkmen_tokenizer.json`

Saklandy tokenizer (√∂wreni≈üden so≈à d√∂redil√Ω√§r).

### `turkmen_hf_tokenizer.json`

Hugging Face formatyndaky tokenizer.

---

## üéØ Ulany≈ü Senaryiyleri

### 1. **Teksti √ñn√º I≈ülemek**

Klassifikasi√Ωa, te≈àdeme, atlar nomini tapma √º√ßin.

```python
tokenizer = TurkmenBPETokenizer()
tokenizer.load("turkmen_tokenizer.json")
tokens = tokenizer.tokenize(my_text)
```

### 2. **Uly Dil Modeli √ñwretmek**

BERT, GPT, T5 modelleri √º√ßin T√ºrkmen texti hazyrlamak.

```python
tokenizer.export_to_huggingface()
# So≈àra Hugging Face 'transformers' kitaphanasy bilen ulan
```

### 3. **Sorag Jogap Sistemi**

Soraq we jogapty tokenla≈üdyrmak.

```python
question = "T√ºrkmenistan'y≈à pa√Ωtagty nedi?"
answer = "A≈ügabat"
q_tokens = tokenizer.tokenize(question)
a_tokens = tokenizer.tokenize(answer)
```

### 4. **Teksti Klassifikasi√Ωa**

Habary, ≈üygry√Ωety, surat √Ωe-de teksti g√∂rn√º≈ülerine b√∂lmek.

---

## üîç Teknika√Ωy Detallary

### BPE Algoritmi Kompleksligini

- **Zaman Kompleksligini**: O(n \* vocab_size), bu √Ωerde n - corpus ozmak
- **√ùaly Kompleksligini**: O(vocab_size)
- **√ñwreni≈ü Sede**: 500 s√∂z Corpus √º√ßin ~30 sekunt (vocab_size=10,000)

### Tokenizersizligi

```
Ba≈ülangy√ß Harp Sany: ~100
Birle≈üdirmeler: 9,887
Jemi Tokenler: ~10,000
√ñwrenilen Atlary: ~150+ (erkek, a√Ωal, ≈ü√§her)
```

---

## ü§ù ≈ûerik Olunasy

T√§ze atlary, ≈ü√§herleri √Ωa-da go≈üulmalary go≈ümak √º√ßin:

```python
# T√§ze atlary go≈üma
tokenizer.add_names(['Beiki', 'T√§√ßberdi'], gender='male')

# T√§ze ≈ü√§herleri go≈üma
tokenizer.add_cities(['Tejen', 'Kaka'])

# Corpus'dan t√§ze modeli √Ωarata
tokenizer.train("new_corpus.txt")
```

---

## üìû Ha√Ωy≈ü we Meseleleri

Problema≈àyz bar √Ωa-da teklip≈àiz bar bolsa, mesele a√ßy≈à!

---

## üìÑ Litsenziya

Bu proje A√ßyk √áe≈üme D√∂wlet Bolyp i≈üle√Ω√§r.
