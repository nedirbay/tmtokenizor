"""
T√ºrkmen Dili √º√ßin BPE (Byte Pair Encoding) Tokenizer
Grammatik d√ºz√ºmleri, adaty atlary we geografik atlary g√∂z √∂≈à√ºnde tut√Ωar
"""

import re
import json
from collections import defaultdict, Counter
from typing import List, Dict, Tuple, Set
import unicodedata

class TurkmenBPETokenizer:
    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = vocab_size
        self.vocab = {}
        self.merges = []
        self.word_freqs = {}
        
        # T√ºrkmen dilini≈à a√Ωratyn harplary
        self.turkmen_chars = set('√§≈à√∂≈ü√º√Ω≈æ√ß')
        
        # T√ºrkmen erkek atlary
        self.male_names = {
            'ahmet', 'muhammet', 'd√∂wlet', 'berdi', 'gurban', 'oraz', 'serdar',
            'atamyrat', 'ba√Ωram', 'gurbanguly', 'g√∂rogly', 'k√§k√§', 'magtymguly',
            'oguz', 'saparmyrat', 't√§√ßmyrat', 'wepa', '√Ωagmyr', '√Ωolaman',
            'merdan', 'rustam', 'nurmuhammet', 'kerim', 'jumamyrat', 'annamuhammet',
            'sapar', 'rejep', 'amanmyrat', 'myrat', 'guwan√ß', 'arslan',
            'batyr', 'g√∂khan', 'huda√Ωberdi', 'm√§mmet', 'nazim', '≈ü√∂hrat',
            '√Ωag≈üy', '√Ωusup', '√Ωusupmyrat', '√Ωusupguly', '√Ωusupgurly',
            'anna', 'muhammetmyrat', 'muhammetguly', 'muhammetnur', 'muhammet√∂wez',
            'muhammetrahman', 'muhammetserdar', 'muhammetta√Ωly', 'muhammet√Ωusup',
            'myrat', 'nurmyrat', '√∂wezmyrat', 'rahmanmyrat', 'serdarmyrat', 't√§√ßmyrat',
            'ta√Ωlymyrat', 'annamyrat', 'amanmyrat', 'gurbangulymyrat', 'guwan√ßmyrat',
            'huda√Ωberdimyrat', 'kerimmyrat', 'nazimmyrat', 'rejepmyrat', 'saparmyrat',
            '≈ü√∂hratmyrat', 'wepamyrat', '√Ωag≈üymyrat', '√Ωusupmyrat', '√Ωusupgulymyrat',
        }
        
        # T√ºrkmen zenan atlary
        self.female_names = {
            'a√Ωna', 'oguljan', 'mahri', 'jennet', 'g√ºll√º', 'g√ºn√§', 'g√ºzel',
            'lale', 'ma√Ωa', 'ogulnabat', 's√§het', 'soltan', 'a√Ωg√∂zel',
            'a√Ωjemal', 'bibig√ºl', 'bibi', 'g√ºlbahar', 'g√ºlnara', 'jahan',
            'le√Ωla', 'maral', 'nazarg√ºl', 'rowa√Ωat', '≈üa√Ωg√ºl', 't√§√ßg√ºl',
            '√Ωyldyz', 'ze√Ωnep', 'g√ºl≈üat', 'mahym', 'ogulnaz'
        }
        
        self.regions = {
            'a≈ügabat', 'ahal', 'balkan', 'da≈üoguz', 'lebap', 'mary', 'arkadag'
        }
        # T√ºrkmenistany≈à ≈ü√§herleri we wela√Ωatlary
        self.cities = {
            # Pa√Ωtagt we D√∂wlet √§hmi√Ωetli ≈ü√§herler
            'a≈ügabat', 'arkadag',
            
            # Wela√Ωat merkezleri
            'anew', '√§new',             # Ahal
            'balkanabat', 'nebitdag',   # Balkan (K√∂ne ady: Nebitdag)
            'da≈üoguz', 'da≈ühowuz',      # Da≈üoguz
            't√ºrkmenabat', '√ß√§rjew',    # Lebap (K√∂ne ady: √á√§rjew)
            'mary',                     # Mary
            
            # Balkan wela√Ωaty ≈ü√§herleri
            't√ºrkmenba≈üy', 'krasnowodsk',
            'hazar', '√ßeleken',
            'gumdag',
            'bereked', 'bereket', 'gazanjyk',
            'gyzylarbat', 'serdar',     # Serdar ≈ü√§herini≈à ady Gyzylarbat boldy, √Ω√∂ne ikisem gerek
            'magtymguly', 'garrygala',
            
            # Da≈üoguz wela√Ωaty ≈ü√§herleri
            'k√∂ne√ºrgen√ß',
            'akdepe',
            'boldumsaz',
            'gubadag',
            'g√∂rogly', 'tagta',
            
            # Lebap wela√Ωaty ≈ü√§herleri
            'kerki', 'atamyrat',        # Atamyrat ady √Ωatyryldy, √Ω√∂ne tekstlerde k√∂p
            'gazojak',
            'magdanly', 'gowurdak',
            'se√Ωdi', 'neftezawodsk',
            'd√§new', 'galkyny≈ü',
            'darganata', 'birata',
            
            # Mary wela√Ωaty ≈ü√§herleri
            'ba√Ωramaly',
            '√Ωol√∂ten',
            'murgap',
            'serhetabat', 'gu≈ügy',
            '≈üatlyk',
            
            # Ahal wela√Ωaty & A≈ügabat d√ºz√ºmi (√∂≈àki ≈ü√§herler)
            'tejen',
            'kaka', 'kaahka',
            'sarahs',
            'b√§herden', 'baharly',
            'g√∂kdepe',
            'abadan', 'b√ºzme√Ωin',       # H√§zir A≈ügabady≈à etraplary, √Ω√∂ne ≈ü√§her h√∂km√ºnde du≈ü√Ωar
            'ar√ßabil'
        }

        self.districts = {
            # --- A≈ügabat ≈ü√§herini≈à etraplary ---
            'bagty√Ωarlyk',
            'berkararlyk',
            'b√ºzme√Ωin',
            'k√∂petdag',
            # √ùatyrylan √Ωa-da birle≈üdirilen etraplar (taryhy tekstler √º√ßin gerek)
            'ar√ßabil', '√ßandybil', 'abadan', 'ruhabat',

            # --- Arkadag ≈ü√§herini≈à etraplary ---
            'kyarizek', 'k√§rizek',
            'gorjaw',

            # --- Ahal wela√Ωaty ---
            'ak bugda√Ω', 'akbugda√Ω',
            'babada√Ωhan',
            'b√§herden', 'baharly',
            'g√∂kdepe',
            'kaka', 'kaahka',
            'sarahs',
            'tejen',

            # --- Balkan wela√Ωaty ---
            'bereket', 'gazanjyk',
            'etrek', 'gyzyletrek',
            'esenguly',
            'magtymguly', 'garrygala',
            'gyzylarbat', 'serdar',
            't√ºrkmenba≈üy',

            # --- Da≈üoguz wela√Ωaty ---
            'akdepe',
            'boldumsaz',
            'g√∂rogly', 'tagta',
            'gubadag',
            'k√∂ne√ºrgen√ß',
            'ruhubelent',
            's.a.ny√Ωazow', 'ny√Ωazow',
            'saparmyrat t√ºrkmenba≈üy', 's.t√ºrkmenba≈üy',

            # --- Lebap wela√Ωaty ---
            '√ß√§rjew', 'serdarabat',     # Serdarabat etraby √á√§rjew boldy
            'darganata', 'birata',
            'd√§new', 'galkyny≈ü',
            'hala√ß',
            'hojambaz',
            'kerki', 'atamyrat',
            'k√∂√Ωtendag', '√ßar≈üa≈à≈ày',
            'sa√Ωat',
            # √ùatyrylan √Ωa-da birle≈üen etraplar (tokenizer √º√ßin saklamak pe√Ωdaly)
            'd√∂wletli', 'farap', 'garashsyzlyk', 'gara≈üsyzlyk', 'sakar', 'be√Ωik t√ºrkmenba≈üy',

            # --- Mary wela√Ωaty ---
            'ba√Ωramaly',
            'garagum',
            'mary',
            'murgap',
            'oguzhan', 'oguz han',
            'sakar√ß√§ge',
            'serhetabat', 'gu≈ügy',
            'tagtabazar',
            't√ºrkmengala',
            'wekilbazar',
            '√Ωol√∂ten',
            # √ùatyrylanlar
            'altyn s√§hra'
        }
        
        # Geografik atlar (da≈üary √Ωurt)
        self.countries = {
            't√ºrki√Ωe', 'e√Ωran', 'russi√Ωa', 'gazagystan', '√∂zbegistan',
            't√§jigistan', 'owganystan', 'hyta√Ω', 'hindistan', 'pakistan',
            'azerba√Ωjan', 'gyrgyzystan', 'germani√Ωa', 'fransi√Ωa', 'angli√Ωa',
            'amerika', 'kanada', 'brazili√Ωa', 'awstrali√Ωa', 'amerikany≈à birle≈üen ≈ütatlary'
        }
        
        # Be√Ωleki m√∂h√ºm s√∂zler (√Ωokary √Ωygylykly)
        self.important_words = {
            't√ºrkmenistan', 't√ºrkmen', 't√ºrkmenistany≈à', 't√ºrkmenleri≈à',
            'gara≈üsyzlyk', 'bitaraplyk', 'prezident', 'halk', 'watan',
            'd√∂wlet', 'respublika', 'mejlis', 'ministr', 'ministrligi'
        }
        
        # T√ºrkmen dilini≈à esasy go≈üulmalary (suffixes)
        self.common_suffixes = [
            # --- 4 Harp we uzynrak ---
            'lary≈à', 'leri≈à', 'syzlyk', 'sizlik', 'darlyk', 'derlik',
            '√ßylyk', '√ßilik', 'k√§rlik', 'gerlik', 
            'jakdyr', 'jekdir', 'maly', 'meli',
            '√Ωarka', '√Ω√§rk√§', '√Ωaka', '√Ω√§k√§',
            'madyk', 'medik', 'maly', 'meli',
            
            # --- 3 Harplylar ---
            'lar', 'ler', 'dan', 'den', 'tan', 'ten',
            'ny≈à', 'ni≈à', 'nu≈à', 'n√º≈à', # E√Ωelik d√º≈ü√ºm (genitive) - Sizi≈à sanawy≈àyzda √Ωokdy
            'da≈ü', 'de≈ü', # Meselem: watan-da≈ü
            'lyk', 'lik', 'luk', 'l√ºk', # At √Ωasa√Ωjylar: g√∂zellik
            'syz', 'siz', # Sypat √Ωasa√Ωjylar
            'dar', 'gir', 'gor', # Meselem: bergidar
            '√Ωar', '√Ω√§r', '√Ωor', '√Ω√∂r', # H√§zirki zaman
            'jak', 'jek', # Geljek zaman
            'my≈ü', 'mi≈ü', # E≈üidilen ge√ßen zaman
            'dyr', 'dir', 'dur', 'd√ºr', # Habar go≈üulmasy (Predicative)
            'man', 'm√§n', # Hal i≈ülik (gelm√§n)
            'maz', 'mez', # Ink√§r geljek zaman
            'mak', 'mek', # I≈ülik d√º√Ωbi (infinitive)
            'yjy', 'iji', 'ujy', '√ºji', # At √Ωasa√Ωjy: oka-yjy
            
            # --- 2 Harplylar ---
            'ny', 'ni', # Tab≈üyry≈ü d√º≈ü√ºm
            'da', 'de', 'ta', 'te', # Wagt-orun d√º≈ü√ºm
            'ym', 'im', 'um', '√ºm', # Meni≈à (I)
            'y≈à', 'i≈à', 'u≈à', '√º≈à', # Seni≈à (II) we E√Ωelik d√º≈ü√ºm gysgalan g√∂rn√º≈üi
            'sy', 'si', # Onu≈à (III)
            'ka', 'k√§', # Sorag/g√ºman: barmyka?
            'my', 'mi', # Sorag: barmy?
            'ma', 'me', # Ink√§r: gelme
            'yp', 'ip', 'up', '√ºp', # Hal i≈ülik: gelip
            'an', 'en', # Sypat i≈ülik: gelen
            'dy', 'di', # ≈ûa√Ωatly ge√ßen zaman
            '√ßy', '√ßi', # K√§r a≈àlad√Ωan: balyk√ßy
            
            # --- 1 Harplylar (Bular i≈à so≈àunda bolmaly) ---
            'a', 'e', '√§', # G√∂n√ºkdirilen d√º≈ü√ºm
            'y', 'i', # Tab≈üyry≈ü d√º≈ü√ºm gysgalan
        ]
        self.common_suffixes = sorted(list(set(self.common_suffixes)), key=len, reverse=True)
        # A√Ωratyn tokenler
        self.special_tokens = {
            # Standart tokenler
            '<pad>': 0,    # Padding (Doldurgy√ß - uzynlygy de≈àlemek √º√ßin)
            '<unk>': 1,    # Unknown (N√§tany≈ü s√∂z)
            '<bos>': 2,    # Beginning of Sentence (Sentensi√Ωa ba≈üy - GPT √º√ßin)
            '<eos>': 3,    # End of Sentence (Sentensi√Ωa so≈ày - GPT √º√ßin)
            
            '<mask>': 4,   # Masked Language Modeling (MLM) √º√ßin. Meselem: "Men <mask> gid√Ω√§rin."
            '<cls>': 5,    # Classification (Tekst klassifikasi√Ωasy √º√ßin ba≈üy)
            '<sep>': 6,    # Separator (Iki s√∂zlemi b√∂lmek √º√ßin. Meselem: Sorag <sep> Jogap)
            
            '<name>': 7,   
            '<city>': 8,   
            '<country>': 9,
            '<num>': 10,   # Sanlary bellemek √º√ßin (islege g√∂r√§)
            '<url>': 11,   # Linkleri bellemek √º√ßin
            '<email>': 12  # E-po√ßtalary bellemek √º√ßin
        }
        
    def is_proper_noun(self, word: str) -> Tuple[bool, str]:
        """
        S√∂zi≈à adaty at (proper noun) bardygyny barla√Ωar
        Ga√Ωtary≈ü: (ha√Ωsy_at_bolsa, at_g√∂rn√º≈üi)
        """
        word_lower = word.lower()
        
        # Adam atlary
        if word_lower in self.male_names or word_lower in self.female_names:
            return True, 'name'
        
        # ≈û√§her atlary
        if word_lower in self.cities:
            return True, 'city'
        
        # √ùurt atlary
        if word_lower in self.countries:
            return True, 'country'
        
        # M√∂h√ºm s√∂zler
        if word_lower in self.important_words:
            return True, 'important'
        
        return False, None
    
    def normalize_text(self, text: str) -> str:
        text = unicodedata.normalize("NFKC", text)
        # 2. T√ºrkmen dilind√§ki √Ωygy du≈ü gel√Ω√§n √Ωal≈ày≈ülary d√ºzetmek
        replacements = {
            "√ø": "√Ω", "¬•": "√Ω",  # √ùal≈ày≈ü kodlanan √Ω-ler
            "…ô": "√§",            # Azeri/Tatar klawiaturasyndan galanlar
            "≈ü": "≈ü", "s": "≈ü",  # K√§wagt ≈ü √Ωerine s √Ωazyl√Ωar (mu≈àa seresap bolmaly)
            "‚Äú": '"', "‚Äù": '"', "‚Äô": "'", "‚Äò": "'", # D√ºrli dyrnaklar
            "\u00ad": "",        # Soft hyphen (g√∂r√ºnme√Ω√§n kese √ßyzyk)
            "&nbsp;": " "        # HTML bo≈üluk
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
            
        # 3. Artykma√ß bo≈üluklary a√Ωyrmak
        text = re.sub(r'\s+', ' ', text)
        return text.strip().lower()

    def aggressive_suffix_split(self, word: str) -> str:
        """
        S√∂zleri≈à so≈àundaky go≈üulmalary b√∂l√Ω√§r. 
        Meselem: "mekdepde" -> "mekdep de"
        """
        # Gysga s√∂zlere degme√Ω√§ris (√Ωal≈ày≈ü b√∂lmezlik √º√ßin)
        if len(word) < 4:
            return word
            
        # Go≈üulmalary uzynlygyna g√∂r√§ tertiple√Ω√§ris (uzynlar √∂≈àde)
        sorted_suffixes = sorted(self.common_suffixes, key=len, reverse=True)
        
        for suffix in sorted_suffixes:
            if word.endswith(suffix):
                # K√∂k s√∂z gaty gysga bolmaly d√§l (meselem: 'ada' -> 'a da' bolmazlygy √º√ßin)
                stem = word[:-len(suffix)]
                if len(stem) >= 2:
                    return f"{stem} {suffix}"
        return word

    # 2-NJI √ÑDIM: pre_tokenize funksi√Ωasyny t√§zel√§≈à (k√∂ne koduny √∂√ß√ºrip, ≈üuny go√Ωu≈à)
    def pre_tokenize(self, text: str) -> List[Tuple[str, str]]:
        """
        Teksti s√∂zlere we ny≈üanlara b√∂l√Ω√§r, at g√∂rn√º≈üini hem belle√Ω√§r.
        Go≈üulmalary hem a√Ωratynla√Ωar.
        """
        text_lower = text.lower()
        
        # Regex pattern
        pattern = r"[a-z√§≈à√∂≈ü√º√Ω≈æ√ß–∞-—è]+|[0-9]+|[^\w\s]+"
        raw_tokens = re.findall(pattern, text_lower)
        
        typed_tokens = []
        for token in raw_tokens:
            # Ilki bilen adaty atdygyny barla
            is_proper, proper_type = self.is_proper_noun(token)
            
            if is_proper:
                typed_tokens.append((token, proper_type))
            else:
                # Eger adaty at d√§l bolsa, go≈üulmany barlap g√∂r
                # Meselem: "mekdepde" -> "mekdep" "de"
                split_version = self.aggressive_suffix_split(token)
                
                if split_version != token:
                    # Eger s√∂z b√∂l√ºnen bolsa (mekdep de)
                    parts = split_version.split()
                    for part in parts:
                        typed_tokens.append((part, 'word'))
                else:
                    # B√∂l√ºnmedik bolsa
                    typed_tokens.append((token, 'word'))
        
        return typed_tokens

    def get_word_frequencies(self, corpus: List[str]) -> Dict[str, int]:
        """
        S√∂zleri≈à √Ωygylyklaryny hasapla√Ωar
        """
        word_freqs = Counter()
        
        for text in corpus:
            tokens = self.pre_tokenize(text)
            # Di≈àe s√∂z b√∂lekleri al, at g√∂rn√º≈üini a√Ωyr
            words = [token for token, _ in tokens]
            word_freqs.update(words)
        
        return dict(word_freqs)
    
    def get_character_vocab(self, word_freqs: Dict[str, int]) -> Set[str]:
        """
        Ba≈ülangy√ß harp toplumyny d√∂red√Ω√§r
        """
        chars = set()
        for word in word_freqs.keys():
            chars.update(list(word))
        return chars
    
    def split_word_to_chars(self, word: str) -> List[str]:
        """
        S√∂zi harp tokenlerine b√∂l√Ω√§r, so≈àky harpa "</w>" go≈ü√Ωar
        """
        if len(word) == 0:
            return []
        chars = list(word[:-1])
        chars.append(word[-1] + '</w>')
        return chars
    
    def get_pair_frequencies(self, splits: Dict[str, List[str]], 
                            word_freqs: Dict[str, int]) -> Dict[Tuple[str, str], int]:
        """
        Go≈à≈üy token j√ºb√ºtlerini≈à √Ωygylyklaryny hasapla√Ωar
        """
        pair_freqs = defaultdict(int)
        
        for word, freq in word_freqs.items():
            split = splits[word]
            if len(split) < 2:
                continue
            
            for i in range(len(split) - 1):
                pair = (split[i], split[i + 1])
                pair_freqs[pair] += freq
        
        return dict(pair_freqs)
    
    def merge_pair(self, pair: Tuple[str, str], splits: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """
        I≈à √Ωygy j√ºb√ºti birle≈üdir√Ω√§r
        """
        new_splits = {}
        
        for word, split in splits.items():
            if len(split) < 2:
                new_splits[word] = split
                continue
            
            new_split = []
            i = 0
            
            while i < len(split):
                if i < len(split) - 1 and (split[i], split[i + 1]) == pair:
                    new_split.append(split[i] + split[i + 1])
                    i += 2
                else:
                    new_split.append(split[i])
                    i += 1
            
            new_splits[word] = new_split
        
        return new_splits
    
    def add_proper_nouns_to_vocab(self, vocab: List[str]) -> List[str]:
        """
        Adaty atlary s√∂z kitabyna go≈ü√Ωar
        """
        # Adam atlary
        for name in self.male_names | self.female_names:
            if name not in vocab:
                vocab.append(name + '</w>')
        
        # ≈û√§her atlary
        for city in self.cities:
            if city not in vocab:
                vocab.append(city + '</w>')
        
        # √ùurt atlary
        for country in self.countries:
            if country not in vocab:
                vocab.append(country + '</w>')
        
        # M√∂h√ºm s√∂zler
        for word in self.important_words:
            if word not in vocab:
                vocab.append(word + '</w>')
        
        return vocab
    
    def train(self, corpus_path: str, verbose: bool = True):
        def corpus_generator():
            with open(corpus_path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        yield line.strip()
        """
        Korpusda BPE tokenizerini √∂wred√Ω√§r
        """
        if verbose:
            print("üáπüá≤ T√ºrkmen BPE Tokenizer √∂wreni≈üi ba≈ülan√Ωar...")
        
        # 1. A√Ωratyn tokenleri go≈ü
        vocab = list(self.special_tokens.keys())
        if verbose:
            print(f"‚úì {len(self.special_tokens)} a√Ωratyn token go≈üuldy")
        
        # 2. S√∂zleri≈à √Ωygylyklaryny hasapla
        self.word_freqs = self.get_word_frequencies(corpus_generator())
        if verbose:
            print(f"‚úì {len(self.word_freqs)} √º√Ωtge≈üik s√∂z tapyldy")
        
        # 3. Ba≈ülangy√ß harp toplumyny d√∂ret
        chars = self.get_character_vocab(self.word_freqs)
        vocab.extend(list(chars) + ['</w>'])
        if verbose:
            print(f"‚úì Ba≈ülangy√ß harp toplumy: {len(chars)} simwol")
        
        # 4. Adaty atlary go≈ü
        vocab = self.add_proper_nouns_to_vocab(vocab)
        if verbose:
            print(f"‚úì Adaty atlar go≈üuldy (adam, ≈ü√§her, √Ωurt atlary)")
            print(f"  - Adam atlary: {len(self.male_names | self.female_names)}")
            print(f"  - ≈û√§her atlary: {len(self.cities)}")
            print(f"  - √ùurt atlary: {len(self.countries)}")
        
        # 5. S√∂zleri harplara b√∂l
        splits = {word: self.split_word_to_chars(word) 
                 for word in self.word_freqs.keys()}
        
        # 6. BPE birle≈üdirmeleri
        num_merges = self.vocab_size - len(vocab)
        
        if verbose:
            print(f"\nüîÑ BPE birle≈üdirmeleri ba≈ülan√Ωar ({num_merges} gezek)...")
        
        for i in range(num_merges):
            # J√ºb√ºt √Ωygylyklary
            pair_freqs = self.get_pair_frequencies(splits, self.word_freqs)
            
            if not pair_freqs:
                if verbose:
                    print(f"‚ö† {i} birle≈üdirmeden so≈à t√§ze j√ºb√ºt tapylmady")
                break
            
            # I≈à √Ωygy j√ºb√ºt
            best_pair = max(pair_freqs, key=pair_freqs.get)
            
            # Birle≈üdir
            splits = self.merge_pair(best_pair, splits)
            self.merges.append(best_pair)
            
            # T√§ze tokeni go≈ü
            new_token = best_pair[0] + best_pair[1]
            vocab.append(new_token)
            
            if verbose and (i + 1) % 500 == 0:
                print(f"  {i + 1}/{num_merges} birle≈üdirme tamamlandy - "
                      f"I≈à so≈àky: {best_pair[0]} + {best_pair[1]} = {new_token}")
        
        # S√∂z kitabyny d√∂ret
        self.vocab = {token: idx for idx, token in enumerate(vocab)}
        
        # A√Ωratyn tokenler ID-lerini t√§zele
        for token, idx in self.special_tokens.items():
            if token in self.vocab:
                # A√Ωratyn tokenleri≈à ID-lerini √º√Ωtget
                old_idx = self.vocab[token]
                self.vocab[token] = idx
                # Be√Ωleki tokenleri≈à ID-lerini d√ºzet
                for t in list(self.vocab.keys()):
                    if self.vocab[t] == idx and t != token:
                        self.vocab[t] = old_idx
        
        if verbose:
            print(f"\n‚úÖ √ñwreni≈ü tamamlandy! Jemi {len(self.vocab)} token")
            self._print_statistics()
    
    def _print_statistics(self):
        """
        Tokenizer statistikasyny g√∂rkez√Ω√§r
        """
        print("\nüìä Statistika:")
        print(f"  - Jemi tokenler: {len(self.vocab)}")
        print(f"  - Jemi birle≈üdirmeler: {len(self.merges)}")
        print(f"  - A√Ωratyn tokenler: {len(self.special_tokens)}")
        
        # Adam atlary
        name_count = sum(1 for name in (self.male_names | self.female_names) 
                        if (name + '</w>') in self.vocab)
        print(f"  - Adam atlary: {name_count}/{len(self.male_names | self.female_names)}")
        
        # ≈û√§her atlary
        city_count = sum(1 for city in self.cities 
                        if (city + '</w>') in self.vocab)
        print(f"  - ≈û√§her atlary: {city_count}/{len(self.cities)}")
        
        # √ùurt atlary
        country_count = sum(1 for country in self.countries 
                           if (country + '</w>') in self.vocab)
        print(f"  - √ùurt atlary: {country_count}/{len(self.countries)}")
        
        # T√ºrkmen go≈üulmalaryny≈à √Ωagda√Ωy
        suffix_count = 0
        for suffix in self.common_suffixes:
            for token in self.vocab.keys():
                if suffix in token and token != suffix:
                    suffix_count += 1
                    break
        
        print(f"  - T√ºrkmen go≈üulmalaryny √∂z i√ßine al√Ωan tokenler: {suffix_count}")
    
    def tokenize(self, text: str) -> List[str]:
        """
        Teksti tokenlere b√∂l√Ω√§r
        """
        tokens = []
        typed_words = self.pre_tokenize(text)
        
        for word, word_type in typed_words:
            # Eger adaty at bolsa we s√∂z kitabynda bar bolsa, tutu≈ü s√∂z h√∂km√ºnde go≈ü
            full_word_token = word + '</w>'
            if word_type != 'word' and full_word_token in self.vocab:
                tokens.append(full_word_token)
                continue
            
            # Harplardan ba≈üla
            word_tokens = self.split_word_to_chars(word)
            
            # Birle≈üdirmeleri ulan
            for merge in self.merges:
                i = 0
                while i < len(word_tokens) - 1:
                    if (word_tokens[i], word_tokens[i + 1]) == merge:
                        word_tokens[i] = word_tokens[i] + word_tokens[i + 1]
                        word_tokens.pop(i + 1)
                    else:
                        i += 1
            
            tokens.extend(word_tokens)
        
        return tokens
    
    def encode(self, text: str) -> List[int]:
        """
        Teksti token ID-lerine √∂w√ºr√Ω√§r
        """
        tokens = self.tokenize(text)
        return [self.vocab.get(token, self.special_tokens['<unk>']) for token in tokens]
    
    def decode(self, token_ids: List[int]) -> str:
        """
        Token ID-lerini tekste √∂w√ºr√Ω√§r
        """
        # ID-den token-e ge√ßi≈ü
        id_to_token = {idx: token for token, idx in self.vocab.items()}
        tokens = [id_to_token.get(id, '<unk>') for id in token_ids]
        
        # A√Ωratyn tokenleri a√Ωyr
        tokens = [t for t in tokens if t not in self.special_tokens]
        
        # Tokenleri birle≈üdir we </w> a√Ωyr
        text = ''.join(tokens).replace('</w>', ' ')
        return text.strip()
    
    def save(self, filepath: str):
        """
        Tokenizerini fa√Ωla sakla√Ωar
        """
        data = {
            'vocab': self.vocab,
            'merges': self.merges,
            'vocab_size': self.vocab_size,
            'special_tokens': self.special_tokens,
            'male_names': list(self.male_names),
            'female_names': list(self.female_names),
            'cities': list(self.cities),
            'countries': list(self.countries),
            'important_words': list(self.important_words)
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úì Tokenizer '{filepath}' fa√Ωlyna saklandy")
    
    def load(self, filepath: str):
        """
        Tokenizerini fa√Ωldan √Ω√ºkle√Ω√§r
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.vocab = data['vocab']
        self.merges = [tuple(merge) for merge in data['merges']]
        self.vocab_size = data['vocab_size']
        self.special_tokens = data.get('special_tokens', self.special_tokens)
        self.male_names = set(data.get('male_names', []))
        self.female_names = set(data.get('female_names', []))
        self.cities = set(data.get('cities', []))
        self.countries = set(data.get('countries', []))
        self.important_words = set(data.get('important_words', []))
        
        print(f"‚úì Tokenizer '{filepath}' fa√Ωlyndan √Ω√ºklendi")
    
    def add_names(self, names: List[str], gender: str = 'male'):
        """
        T√§ze atlary go≈ümak (√∂wreni≈üden so≈à ulanmak √º√ßin)
        """
        if gender == 'male':
            self.male_names.update([n.lower() for n in names])
        else:
            self.female_names.update([n.lower() for n in names])
        
        # S√∂z kitabyna go≈ü
        for name in names:
            name_token = name.lower() + '</w>'
            if name_token not in self.vocab:
                self.vocab[name_token] = len(self.vocab)
        
        print(f"‚úì {len(names)} t√§ze at go≈üuldy")
    
    def add_cities(self, cities: List[str]):
        """
        T√§ze ≈ü√§her atlaryny go≈ümak
        """
        self.cities.update([c.lower() for c in cities])
        
        for city in cities:
            city_token = city.lower() + '</w>'
            if city_token not in self.vocab:
                self.vocab[city_token] = len(self.vocab)
        
        print(f"‚úì {len(cities)} t√§ze ≈ü√§her ady go≈üuldy")
    
    def export_to_huggingface(self, save_path: str = "turkmen_hf_tokenizer.json"):
        """
        Hugging Face 'tokenizers' formatyna ge√ßir√Ω√§r we sakla√Ωar.
        Uly modeller (BERT, GPT) bilen ulanmak √º√ßin.
        """
        try:
            from tokenizers import Tokenizer, models, pre_tokenizers, decoders
        except ImportError:
            print("‚ùå Bu funksi√Ωa √º√ßin 'tokenizers' kitaphanasy gerek.")
            print("Ha√Ωy≈ü, 'pip install tokenizers' bu√Ωrugyny √Ωerine √Ωetiri≈à.")
            return

        print("üîÑ Hugging Face formatyna ge√ßiril√Ω√§r...")
        
        # Vocab (sozluk) dict formatyndan list formatyna ge√ßirmek zerur bolup biler, 
        # √Ω√∂ne HF BPE modeli g√∂ni dict kabul ed√Ω√§r (token -> id).
        # Bizi≈à 'merges' listimiz tuple (a, b), √Ω√∂ne HF string "a b" isle√Ω√§r.
        
        hf_merges = [f"{p[0]} {p[1]}" for p in self.merges]
        
        # T√§ze tokenizer d√∂retmek
        # Unknown token h√∂km√ºnde <unk> ulan√Ωarys
        hf_tokenizer = Tokenizer(models.BPE(vocab=self.vocab, merges=hf_merges, unk_token="<unk>"))
        
        # Tokenizer sazlamalary (Pre-tokenizer)
        # Bizi≈à Python kodumyzda ed√Ω√§n whitespace b√∂lmegimizi ga√Ωtalamaly
        hf_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
        
        # Saklamak
        hf_tokenizer.save(save_path)
        print(f"‚úÖ Hugging Face tokenizer '{save_path}' fa√Ωlyna saklandy!")


# Ulany≈ü mysaly
if __name__ == "__main__":

    print("=" * 60)
    tokenizer = TurkmenBPETokenizer(vocab_size=1000)
    tokenizer.train("dataset_AB_220524.txt", verbose=True)
    
    # Synaglary ge√ßir
    print("\n" + "=" * 60)
    print("üß™ SYNAG MYSALLARY")
    print("=" * 60)
    
    test_texts = [
        "Ahmet A≈ügabatda i≈üle√Ω√§r",
        "Oguljan Mary ≈ü√§herinden geldi",
        "T√ºrkmenistan we T√ºrki√Ωe dostlukly √Ωurtlar",
        "Serdar Da≈üoguzdan T√ºrkmenabada bardy",
        "Magtymguly Pyragyny≈à ≈üygry√Ωeti"
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nüìù Synag {i}:")
        print(f"Giri≈ü: {text}")
        
        tokens = tokenizer.tokenize(text)
        print(f"Tokenler: {tokens}")
        print(f"Token sany: {len(tokens)}")
        
        encoded = tokenizer.encode(text)
        print(f"Kodlanan: {encoded}")
        
        decoded = tokenizer.decode(encoded)
        print(f"Dekodlanan: {decoded}")
        
        # At analizi
        typed_tokens = tokenizer.pre_tokenize(text)
        names = [t for t, typ in typed_tokens if typ == 'name']
        cities = [t for t, typ in typed_tokens if typ == 'city']
        
        if names:
            print(f"üßë Adam atlary: {names}")
        if cities:
            print(f"üèôÔ∏è ≈û√§her atlary: {cities}")
    
    # Sakla
    print("\n" + "=" * 60)
    tokenizer.save("turkmen_tokenizer.json")
    tokenizer.export_to_huggingface("turkmen_hf_tokenizer.json")

    # T√§ze atlar go≈ümak mysaly
    print("\n" + "=" * 60)